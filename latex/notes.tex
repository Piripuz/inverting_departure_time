\documentclass{article}

\usepackage[english]{babel}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{siunitx}
\usepackage{float}
\usepackage[thinc]{esdiff}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{booktabs}
\usepackage{minted}

\usepackage[pdfborderstyle={/S/U/W 0}]{hyperref}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}

\section{Generating data}

\subsection{Intro}

The cost function, in function of departure time \(t_d\), is
\begin{equation}
  \label{eq:cost_td}
  C(t_d) = \alpha tt(t_d) + \beta[t^*-t_d-tt(t_d)]^+ + \gamma[t_d+tt(t_d)-t^*]^+ 
\end{equation}
where
\begin{itemize}
\item \(t^*\) is the desired arrival time
\item \(\alpha\) is the value of time spent travelling
\item \(\beta\) is the value of time spent waiting there
\item \(\gamma\) is the value of time arriving late
\item \(tt(t_d)\) is the time spent travelling if leaving at time \(t_d\)
\item \([x]^+ = \max(0, x)\)
\end{itemize}

\subsection{Are $t_a$ and $t_d$ equivalent?}

It would be helpful to express the cost function in term of the arrival time \(t_a = t_d + tt(t_d)\),
so that the second part of the cost function would greatly simplify
\[ \beta[t^*-t_d-tt(t_d)]^+ + \gamma[t_d+tt(t_d)-t^*]^+ = \beta[t^*-t_a]^+ + \gamma[t_a-t^*]^+ \]

But how do we express the first term \(tt(t_d)\) in terms of \(t_a\)?

\begin{equation}
  \label{eq:ta_td}
  t_a(t_d) = t_d + tt(t_d)
\end{equation}

Note that the travel time can be expressed in terms of the arrival time \(t_a\) if and only if the function
\eqref{eq:ta_td} is invertible:
this is because the departure time can be easily reconstructed from the travel time and the arrival time (and viceversa).

\subsubsection{When are they equivalent?}

Inverting \eqref{eq:ta_td} is not analytically possible a priori, and may not be possible in general.
It only depends on whether the function \(tt(t_d)\) ever grows more than the identity.
For instance, if the travel time is a gausssian
\begin{equation*}
  tt(t_d) = \frac{1}{\sigma\sqrt{2\pi}}\exp\left(-{\frac{t_d^2}{2\sigma^2}}\right)
\end{equation*}
then the arrival time is invertible if and only if the variance \(\sigma\) satisfies the condition
\[\sigma \geq (2\pi e)^{-\frac{1}{4}} \approx 0.492 \]

In general, assuming that the function grows less than the identity is pretty reasonable,
since in real world I think leaving later results in arriving later.

From now on, assume \eqref{eq:ta_td} is invertible, and \(tt_a(t_a) = tt(t_d(t_a))\) exists.
Moreover, \(\alpha = 1\).

\subsection{Where is the cost minimized?}

\begin{equation}
  \label{eq:cost_ta}
  C(t_a) = tt_a(t_a) + \beta[t^*-t_a]^+ + \gamma[t_a-t^*]^+
\end{equation}

The cost function \eqref{eq:cost_ta} could be minimized either at the only non differentiable point (for \(t_a = t^*\))
or at one of the points where its derivative is zero.
\begin{equation}
  \label{eq:cost_diff}
  C'(t_a) =
  \begin{cases}
    tt_a'(t_a) -\beta \quad &\text{if } t_a < t^* \\
    tt_a'(t_a) + \gamma \quad &\text{otherwise}
  \end{cases}
\end{equation}

Setting it equal to zero, the minimum is realized by one of the points

\begin{align*}
  & t_a |\ tt_a'(t_a) = \beta, t_a < t^* \\
  & t_a = t^* \\
  & t_a |\ tt_a'(t_a) = -\gamma, t_a > t^* \\
\end{align*}

\subsubsection{If travel time is 1-lipschitz then we have at most 2 minima}

Note that, here, assuming

\begin{equation}
  \label{eq:cond_late_min}
|tt_a'(t_a)| < 1 \ \forall t_a, \qquad \gamma > \alpha = 1
\end{equation}
implies that there is no time that satisfies the last condition, that means that arriving late never realises a (not even local) minimum.

These assumptions are reasonable for what discussed earlier, but \textcolor{red}{this should be looked at better}.

\subsubsection{Gaussian travel time}

Again, let's look at the case in which the travel time is gaussian.
I assume that the travel time is gaussian in function of the arrival time as well,
and this can be justified by saying that
\[t_a - t_d >> \max_t | tt'(t)|\]
but \textcolor{red}{I doubt that this is an assumption that can be made}.
Still, assume that
\begin{equation}
  \label{eq:travel_time_gauss}
  tt_a(t_a) = \frac{1}{\sigma\sqrt{2\pi}}\exp\left(-{\frac{t_a^2}{2\sigma^2}}\right)
\end{equation}
and so
\begin{equation}
  \label{eq:travel_time_gauss_diff}
  tt_a'(t_a) = -\frac{t_a}{\sigma^3\sqrt{2\pi}}\exp\left(-{\frac{t_a^2}{2\sigma^2}}\right)
\end{equation}
This can't be inverted analytically and the solutions to the equations above can be found numerically.
(I hope other approaches for minimizing the cost are possible, but nothing comes to my mind right now).

\subsubsection{In general, how to find the minima}


In order to find the minimum of the function \(C(t_a)\) there are thus two possible procedures (that are actually the same one...).
\begin{itemize}
\item Running a root finding algorithm on the functions \(tt_a'(t_a) - \beta,\ tt_a'(t_a) + \gamma\) to find at most two local minima, to be compared to the one potentially realized by \(t^*\) to find the minimum
\item Directly running an optimizer on the cost function \(C(t_a)\). This would find one of the local minima.
  Initializing two optimizers for a very high value and a very low value would find the two minima found above.
  Moreover, in the case in which conditions \eqref{eq:cond_late_min} hold,
  it is enough to launch one optimizer for a very little value.
  This could be the best way, since it is simpler and equally effective (and potentially delegates the computation of an explicit expression for \(tt'\) to an automatic differentiation framework).
\end{itemize}

I will thus implement the second point.

\section{Is the optimum monotonous in $t^*$?}

In particular, let's again assume the travel time to be 1-Lipschitz (so that the minimum of the cost function can never be realized by \(t > t^*\)), and \(C^1(\mathbb{R})\)
on top of going asymptotically to zero as the arrival time goes to \(\infty\) and \(-\infty\).

As discussed earlier, the cost will be minimized either for a point for which \(tt_a'(t_a) = \beta\) or for \(t_a = t^*\).

Consider thus the sets
\begin{align*}
  B & = \{t_a | tt_a'(t_a) = \beta, tt_a'' > 0\} \\
  B_{t^*}&  = \{t_a | tt_a'(t_a) = \beta, t_a \leq t^*\}
\end{align*}

\subsection{Where the travel time function is increasing}

First of all, consider the intervals in which the travel time function \(tt_a\) is increasing.

Here, new potential optima can be created.
The following lemma shows that every time a potential optimum is created (when the set \(B_{t^*} \) grows), the function will locally be constant.

\begin{lemma}
  Let \(t_b \in B\). Then, \(\exists \delta > 0 |\ t_a(t^*) = t_b \ \forall t^* \in [t_b, t_b + \delta)\)
\end{lemma}
\begin{proof}
  Let \(t_0 = t_b + \epsilon\), such that \(tt'(t^*) > \beta\ \forall\ t^* \in (t_b, t_0)\).

  The function to be minimized will be
  \begin{equation*}
    C[t^* = t_0](t_a) = tt_a(t_a) + \beta(t_0 - t_a)
  \end{equation*}
  and will be minimized either for \(t_a = t_0\) or for \(t_a = t_b\).

  But, since the derivative is lower bounded,

  \begin{align*}
    C[t^*=t_0](t_0) & = tt_a(t_0) \\
    & = tt_a(t_b + \epsilon) \\
    & = tt_a(t_b) + \int_{t_b}^{t_0} tt'(t) dt \\
    & \geq tt_a(t_b) + \epsilon\beta \\
    & > tt_a(t_b) \\
    & = C[t^* = t_0](t_b)
  \end{align*}

  The minimum is thus constant for each admissible value of \(\epsilon\), and such an epsilon can be chosen by continuity of the function \(tt_a'\).
\end{proof}

The following lemma shows that the function can never \textit{jump back}:

\begin{lemma}
  Let \(t_b \in B\), \(t_0 > t_b\) such that \(t_a(t_0) = t_0\).
  
  Then, \(\nexists \delta > 0 |\ t_a(t_0 + \delta) = t_b\),
\end{lemma}
\begin{proof}
  There are two cases: either \(tt_a'(t_0) \geq \beta\) or \(tt_a'(t_0) < \beta\).

  If  \(tt_a'(t_0) < \beta\), then the minimum realized  by \(t_0\) is growing slower than the minimum realized by \(t_a\): we can reduce \(t_0\), until we fall in the other case. We must fall in the other case, since \(t_b \in B\)

  If \(tt_a'(t_0) \geq \beta\), it is impossible that the hypothesis \(t_a(t_0) = t_0\) is satisfied:
  each time that \(tt_a'(t_0) \geq \beta\), for what said earlier the function is constant, and not linear.
\end{proof}

As long as the function \(tt\) is increasing, the only admissible jumps are upwards (from a constant function to the identity).

\subsection{Where the travel time function is decreasing}

Where the function is decreasing, no new candidates for the minima are possible, since \(\beta > 0\).

Let thus \(t = \hat{t}\).

If \(t_a(t) = t\), then as long as the function \(tt\) is decreasing the optimal time will grow linearly.

Otherwise, if \(t_a(t) = t_b \neq t\), then the function will remain constant until \(tt_a(t) = \beta (t - t_b)\),
and then jump and grow, again, as the identity.

\end{document}
%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: t
%%% End:
